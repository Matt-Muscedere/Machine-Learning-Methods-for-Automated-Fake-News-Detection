{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71208521-3954-49b4-b87d-8dd68825d70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019d9361-4905-4b10-94b7-703411bdfe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load the Datasets\n",
    "# Load all CSV files from Data/Train and Data/Test directories and combine them into single DataFrames.\n",
    "train_files = glob.glob(os.path.join(\"Data\", \"Train\", \"*.csv\"))\n",
    "test_files = glob.glob(os.path.join(\"Data\", \"Test\", \"*.csv\"))\n",
    "\n",
    "if len(train_files) == 0:\n",
    "    raise FileNotFoundError(\"No training CSV files found in Data/Train\")\n",
    "if len(test_files) == 0:\n",
    "    raise FileNotFoundError(\"No testing CSV files found in Data/Test\")\n",
    "\n",
    "train_df = pd.concat([pd.read_csv(f) for f in train_files], ignore_index=True)\n",
    "test_df = pd.concat([pd.read_csv(f) for f in test_files], ignore_index=True)\n",
    "\n",
    "print(\"Training dataset shape:\", train_df.shape)\n",
    "print(\"Testing dataset shape:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f239d3-504f-43d7-84e0-b06fb4e604ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle dataset shape: (20800, 5)\n",
      "ISOT dataset shape: (44898, 5)\n",
      "Combined dataset shape: (65698, 7)\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Handle Missing Values\n",
    "# Since the CSV files only have 'Statement' and 'label', drop rows missing 'Statement'.\n",
    "train_df = train_df.dropna(subset=['Statement'])\n",
    "test_df = test_df.dropna(subset=['Statement'])\n",
    "\n",
    "print(\"Processed Training dataset shape:\", train_df.shape)\n",
    "print(\"Processed Testing dataset shape:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d20b9f0-87c5-4f9a-8076-d095f06fce51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Extract Features and Labels\n",
    "# Use 'Statement' as the input text and 'label' as the target.\n",
    "X_train = train_df['Statement']\n",
    "y_train = train_df['label']\n",
    "\n",
    "X_test = test_df['Statement']\n",
    "y_test = test_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc44d144-d471-48b3-8b4f-d75201b6e22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Preprocess Text for the LSTM\n",
    "# Tokenization and vocabulary building.\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "    return tokens\n",
    "\n",
    "# Build a vocabulary from the training texts.\n",
    "all_tokens = []\n",
    "for text in X_train:\n",
    "    all_tokens.extend(tokenize(text))\n",
    "\n",
    "# Limit vocabulary to the 10,000 most common words.\n",
    "max_vocab = 10000\n",
    "freq = Counter(all_tokens)\n",
    "vocab = {word: i+2 for i, (word, count) in enumerate(freq.most_common(max_vocab))}\n",
    "vocab_size = len(vocab) + 2  # Add 2 for reserved tokens: 0 for padding, 1 for unknown.\n",
    "\n",
    "def text_to_sequence(text, vocab):\n",
    "    tokens = tokenize(text)\n",
    "    return [vocab.get(token, 1) for token in tokens]\n",
    "\n",
    "# Convert texts to sequences of token indices.\n",
    "X_train_seq = [text_to_sequence(text, vocab) for text in X_train]\n",
    "X_test_seq = [text_to_sequence(text, vocab) for text in X_test]\n",
    "\n",
    "# Set fixed maximum sequence length.\n",
    "max_len = 300\n",
    "\n",
    "def pad_sequence(seq, max_len):\n",
    "    if len(seq) < max_len:\n",
    "        return seq + [0] * (max_len - len(seq))\n",
    "    else:\n",
    "        return seq[:max_len]\n",
    "\n",
    "# Apply padding/truncation.\n",
    "X_train_pad = [pad_sequence(seq, max_len) for seq in X_train_seq]\n",
    "X_test_pad = [pad_sequence(seq, max_len) for seq in X_test_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9d5ddc-cf55-47ce-a334-5ca6ccb87e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Convert Sequences to Torch Tensors\n",
    "X_train_tensor = torch.tensor(X_train_pad, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test_pad, dtype=torch.long)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86077f53-1f35-4c12-9994-557155e9eacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Create a PyTorch Dataset and DataLoader (Initial Version)\n",
    "# Define a basic dataset (without augmentation) for splitting purposes.\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "\n",
    "batch_size = 16\n",
    "# Create datasets for training and testing.\n",
    "train_dataset = NewsDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = NewsDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoader for test data.\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2397f13f-e143-42cf-895d-d5c1d8ebc543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Create a Validation Split from the Training Dataset\n",
    "total_train = len(train_dataset)\n",
    "val_size = int(0.2 * total_train)\n",
    "train_size = total_train - val_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoader for training and validation sets.\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"New train set size: {len(train_dataset)}, Validation set size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fc0ca9-236b-46c5-a4ec-b9c8075a3341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New train set size: 42022, Validation set size: 10505\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Augmentation Setup and Redefinition of the NewsDataset with Augmentation\n",
    "def augment_sequence(seq, drop_prob=0.1):\n",
    "    # Randomly replace tokens (except padding tokens) with the unknown token (index 1).\n",
    "    return [token if token == 0 or np.random.rand() > drop_prob else 1 for token in seq]\n",
    "\n",
    "# Redefine the NewsDataset to support augmentation.\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, augment=False, drop_prob=0.1):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.augment = augment\n",
    "        self.drop_prob = drop_prob\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.augment:\n",
    "            text_list = text.tolist()\n",
    "            text_list = augment_sequence(text_list, drop_prob=self.drop_prob)\n",
    "            text = torch.tensor(text_list, dtype=torch.long)\n",
    "        return text, label\n",
    "\n",
    "# Re-create datasets: enable augmentation for training, disable for validation and testing.\n",
    "train_dataset = NewsDataset(X_train_tensor, y_train_tensor, augment=True, drop_prob=0.1)\n",
    "# For validation, we use the original training data split (without augmentation).\n",
    "# Here, we create a new validation split from the training dataset.\n",
    "total_train = len(train_dataset)\n",
    "val_size = int(0.2 * total_train)\n",
    "train_size = total_train - val_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders with the new datasets.\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f8b5cb-a710-4317-b79a-bfabb77b9006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Pretrain the LSTM Autoencoder\n",
    "class LSTMAutoencoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, latent_dim):\n",
    "        super(LSTMAutoencoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.encoder = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc_enc = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_dec = nn.Linear(latent_dim, embedding_dim)\n",
    "        self.decoder = nn.LSTM(embedding_dim, embedding_dim, batch_first=True)\n",
    "        self.output_layer = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        _, (hidden, _) = self.encoder(embedded)\n",
    "        latent = self.fc_enc(hidden[-1])\n",
    "        hidden_dec = self.fc_dec(latent).unsqueeze(0)\n",
    "        cell_dec = torch.zeros_like(hidden_dec)\n",
    "        dec_out, _ = self.decoder(embedded, (hidden_dec, cell_dec))\n",
    "        reconstructed = self.output_layer(dec_out)\n",
    "        return latent, reconstructed\n",
    "\n",
    "# Hyperparameters for pretraining.\n",
    "pretrain_epochs = 10\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "embedding_dim = 100\n",
    "hidden_dim = 128\n",
    "latent_dim = 64\n",
    "\n",
    "autoencoder = LSTMAutoencoder(vocab_size, embedding_dim, hidden_dim, latent_dim)\n",
    "autoencoder.to(device)\n",
    "\n",
    "criterion_recon = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer_ae = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "best_recon_loss = float('inf')\n",
    "\n",
    "for epoch in range(pretrain_epochs):\n",
    "    autoencoder.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for texts, _ in train_loader:  # Labels not needed for autoencoder pretraining.\n",
    "        texts = texts.to(device)\n",
    "        optimizer_ae.zero_grad()\n",
    "        _, reconstructed = autoencoder(texts)\n",
    "        loss = criterion_recon(reconstructed.view(-1, vocab_size), texts.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer_ae.step()\n",
    "        running_loss += loss.item() * texts.size(0)\n",
    "    \n",
    "    avg_loss = running_loss / len(train_dataset)\n",
    "    print(f\"Autoencoder Pretrain Epoch {epoch+1}/{pretrain_epochs} - Loss: {avg_loss:.4f}\")\n",
    "    if avg_loss < best_recon_loss:\n",
    "        best_recon_loss = avg_loss\n",
    "        best_autoencoder_state = autoencoder.state_dict()\n",
    "\n",
    "autoencoder.load_state_dict(best_autoencoder_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f54e24-a93a-4ad7-a1b5-c830724f4a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder Pretrain Epoch 1/10 - Loss: 0.5910\n",
      "Autoencoder Pretrain Epoch 2/10 - Loss: 0.0019\n",
      "Autoencoder Pretrain Epoch 3/10 - Loss: 0.0003\n",
      "Autoencoder Pretrain Epoch 4/10 - Loss: 0.0001\n",
      "Autoencoder Pretrain Epoch 5/10 - Loss: 0.0000\n",
      "Autoencoder Pretrain Epoch 6/10 - Loss: 0.0000\n",
      "Autoencoder Pretrain Epoch 7/10 - Loss: 0.0000\n",
      "Autoencoder Pretrain Epoch 8/10 - Loss: 0.0000\n",
      "Autoencoder Pretrain Epoch 9/10 - Loss: 0.0000\n",
      "Autoencoder Pretrain Epoch 10/10 - Loss: 0.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 11: Twoâ€‘Phase Joint Training with Validation, Early Stopping, and Alpha Scheduling\n",
    "# Define the joint model that combines an LSTM autoencoder with a classifier.\n",
    "class JointLSTMAutoencoderClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, latent_dim, output_dim):\n",
    "        super(JointLSTMAutoencoderClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.encoder = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc_enc = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_dec = nn.Linear(latent_dim, embedding_dim)\n",
    "        self.decoder = nn.LSTM(embedding_dim, embedding_dim, batch_first=True)\n",
    "        self.output_layer = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        _, (hidden, _) = self.encoder(embedded)\n",
    "        latent = self.fc_enc(hidden[-1])\n",
    "        class_output = self.classifier(latent)\n",
    "        hidden_dec = self.fc_dec(latent).unsqueeze(0)\n",
    "        cell_dec = torch.zeros_like(hidden_dec)\n",
    "        dec_out, _ = self.decoder(embedded, (hidden_dec, cell_dec))\n",
    "        reconstructed = self.output_layer(dec_out)\n",
    "        return latent, reconstructed, class_output\n",
    "\n",
    "# Update latent dimension if needed.\n",
    "latent_dim = 128\n",
    "\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "embedding_dim = 100\n",
    "hidden_dim = 128\n",
    "output_dim = 2  # Binary classification\n",
    "\n",
    "joint_model = JointLSTMAutoencoderClassifier(vocab_size, embedding_dim, hidden_dim, latent_dim, output_dim)\n",
    "joint_model.to(device)\n",
    "\n",
    "#=== Phase 1: Freeze encoder and embedding ===#\n",
    "for param in joint_model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "joint_model.embedding.weight.requires_grad = False\n",
    "\n",
    "criterion_recon = nn.CrossEntropyLoss(ignore_index=0)\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "optimizer_joint = optim.Adam(joint_model.parameters(), lr=0.001)\n",
    "\n",
    "patience = 3\n",
    "best_val_acc = 0.0\n",
    "patience_counter = 0\n",
    "\n",
    "phase1_epochs = 5 \n",
    "phase2_epochs = 5 \n",
    "initial_alpha = 0.5\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "train_acc_list = []\n",
    "val_acc_list = []\n",
    "\n",
    "total_epochs = phase1_epochs + phase2_epochs\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    if epoch == phase1_epochs:\n",
    "        print(\"Unfreezing encoder and embedding layer for Phase 2...\")\n",
    "        for param in joint_model.encoder.parameters():\n",
    "            param.requires_grad = True\n",
    "        joint_model.embedding.weight.requires_grad = True\n",
    "    \n",
    "    current_alpha = initial_alpha * (0.9 ** epoch)\n",
    "    \n",
    "    joint_model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    for texts, labels in train_loader:\n",
    "        texts = texts.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer_joint.zero_grad()\n",
    "        latent, reconstructed, class_output = joint_model(texts)\n",
    "        loss_recon = criterion_recon(reconstructed.view(-1, vocab_size), texts.view(-1))\n",
    "        loss_cls = criterion_cls(class_output, labels)\n",
    "        loss = loss_recon + current_alpha * loss_cls\n",
    "        loss.backward()\n",
    "        optimizer_joint.step()\n",
    "        running_loss += loss.item() * texts.size(0)\n",
    "        _, predicted = torch.max(class_output, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_loss = running_loss / total_train\n",
    "    train_acc = correct_train / total_train\n",
    "    train_loss_list.append(train_loss)\n",
    "    train_acc_list.append(train_acc)\n",
    "    print(f\"Epoch {epoch+1}/{total_epochs} - Train Loss: {train_loss:.4f} - Train Acc: {train_acc:.4f} - Alpha: {current_alpha:.4f}\")\n",
    "    \n",
    "    joint_model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in val_loader:\n",
    "            texts = texts.to(device)\n",
    "            labels = labels.to(device)\n",
    "            _, _, class_output = joint_model(texts)\n",
    "            loss_val = criterion_cls(class_output, labels)\n",
    "            val_loss += loss_val.item() * texts.size(0)\n",
    "            _, predicted_val = torch.max(class_output, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted_val == labels).sum().item()\n",
    "    \n",
    "    avg_val_loss = val_loss / total_val\n",
    "    val_acc = correct_val / total_val\n",
    "    val_loss_list.append(avg_val_loss)\n",
    "    val_acc_list.append(val_acc)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f} - Validation Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        best_model_state = joint_model.state_dict()\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30146771-ed1e-405e-a9b3-5c34630097dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Plot Training Curves and Evaluate on Test Data\n",
    "epochs_range = range(1, len(train_loss_list) + 1)\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, train_loss_list, label='Train Loss')\n",
    "plt.plot(epochs_range, val_loss_list, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, train_acc_list, label='Train Accuracy')\n",
    "plt.plot(epochs_range, val_acc_list, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "joint_model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for texts, labels in test_loader:\n",
    "        texts = texts.to(device)\n",
    "        labels = labels.to(device)\n",
    "        _, _, class_output = joint_model(texts)\n",
    "        _, predicted = torch.max(class_output, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "prec = precision_score(all_labels, all_preds, average='weighted')\n",
    "rec = recall_score(all_labels, all_preds, average='weighted')\n",
    "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "print(\"Joint Model Test Accuracy:\", acc)\n",
    "print(\"Joint Model Test Precision:\", prec)\n",
    "print(\"Joint Model Test Recall:\", rec)\n",
    "print(\"Joint Model Test F1-Score:\", f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env1)",
   "language": "python",
   "name": "env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

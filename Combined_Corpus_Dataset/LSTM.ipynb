{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48330b27-410b-41a2-9f82-6d3440cc3ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa307f1-6715-4b4d-be23-be992a328a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load the Datasets\n",
    "# Load all CSV files from Data/Train and Data/Test directories and combine them into single DataFrames.\n",
    "train_files = glob.glob(os.path.join(\"Data\", \"Train\", \"*.csv\"))\n",
    "test_files = glob.glob(os.path.join(\"Data\", \"Test\", \"*.csv\"))\n",
    "\n",
    "if len(train_files) == 0:\n",
    "    raise FileNotFoundError(\"No training CSV files found in Data/Train\")\n",
    "if len(test_files) == 0:\n",
    "    raise FileNotFoundError(\"No testing CSV files found in Data/Test\")\n",
    "\n",
    "train_df = pd.concat([pd.read_csv(file) for file in train_files], ignore_index=True)\n",
    "test_df = pd.concat([pd.read_csv(file) for file in test_files], ignore_index=True)\n",
    "\n",
    "print(\"Training dataset shape:\", train_df.shape)\n",
    "print(\"Testing dataset shape:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba1e7cd-e2d0-487e-a84b-8215f723ab53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle dataset shape: (20800, 5)\n",
      "ISOT dataset shape: (44898, 5)\n",
      "Combined dataset shape: (65698, 7)\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Handle Missing Values\n",
    "# For both training and testing datasets, drop rows missing the 'Statement' column.\n",
    "train_df = train_df.dropna(subset=['Statement'])\n",
    "test_df = test_df.dropna(subset=['Statement'])\n",
    "\n",
    "print(\"Processed Training dataset shape:\", train_df.shape)\n",
    "print(\"Processed Testing dataset shape:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d40631b-7d76-4d96-8e64-b43e4cd28ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Extract Features and Labels\n",
    "# Use 'Statement' as the input text and 'label' as the target.\n",
    "X_train = train_df['Statement']\n",
    "y_train = train_df['label']\n",
    "\n",
    "X_test = test_df['Statement']\n",
    "# If test CSVs include labels, use them; otherwise, evaluation will use predictions only.\n",
    "y_test = test_df['label'] if 'label' in test_df.columns else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9f81bc4-7479-44dc-bb5d-b95e142cc953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Preprocess Text for the LSTM\n",
    "def tokenize(text):\n",
    "    # Convert text to lowercase and extract word tokens.\n",
    "    text = text.lower()\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "    return tokens\n",
    "\n",
    "# Build vocabulary from the training statements.\n",
    "all_tokens = []\n",
    "for text in X_train:\n",
    "    all_tokens.extend(tokenize(text))\n",
    "    \n",
    "freq = Counter(all_tokens)\n",
    "# Reserve index 0 for padding and 1 for unknown tokens.\n",
    "vocab = {word: i+2 for i, (word, count) in enumerate(freq.items())}\n",
    "vocab_size = len(vocab) + 2\n",
    "\n",
    "def text_to_sequence(text, vocab):\n",
    "    tokens = tokenize(text)\n",
    "    return [vocab.get(token, 1) for token in tokens]\n",
    "\n",
    "# Convert statements into sequences of integers.\n",
    "X_train_seq = [text_to_sequence(text, vocab) for text in X_train]\n",
    "X_test_seq = [text_to_sequence(text, vocab) for text in X_test]\n",
    "\n",
    "# Define the maximum sequence length.\n",
    "max_len = 500\n",
    "\n",
    "def pad_sequence(seq, max_len):\n",
    "    if len(seq) < max_len:\n",
    "        return seq + [0]*(max_len - len(seq))\n",
    "    else:\n",
    "        return seq[:max_len]\n",
    "\n",
    "# Apply padding or truncation.\n",
    "X_train_pad = [pad_sequence(seq, max_len) for seq in X_train_seq]\n",
    "X_test_pad = [pad_sequence(seq, max_len) for seq in X_test_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94eb5946-b06a-40f5-8823-972f6e32eb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Convert Sequences to Torch Tensors\n",
    "X_train_tensor = torch.tensor(X_train_pad, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test_pad, dtype=torch.long)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "if y_test is not None:\n",
    "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e539c447-9a60-4872-bed9-8f3b4616ad15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Create a PyTorch Dataset and DataLoader\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "\n",
    "batch_size = 64\n",
    "train_dataset = NewsDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "if y_test is not None:\n",
    "    test_dataset = NewsDataset(X_test_tensor, y_test_tensor)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5198ec2-8fd5-48cb-a33d-602885bd8ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Define the LSTM Model\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        last_hidden = lstm_out[:, -1, :]\n",
    "        return self.fc(last_hidden)\n",
    "\n",
    "embedding_dim = 100\n",
    "hidden_dim = 128\n",
    "output_dim = 2  # Adjust if you have a different number of classes\n",
    "\n",
    "model = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Set the device (using 'cuda:2' if available, otherwise CPU)\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19deb77-cf6f-4e50-b5b2-8f2995898d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Train the Model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for texts, labels in train_loader:\n",
    "        texts = texts.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * texts.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f} - Accuracy: {epoch_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b33374-d198-4838-9f92-a7c5bd388855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Loss: 0.6655 - Accuracy: 0.5898\n",
      "Epoch 2/5 - Loss: 0.6697 - Accuracy: 0.5737\n",
      "Epoch 3/5 - Loss: 0.4905 - Accuracy: 0.7611\n",
      "Epoch 4/5 - Loss: 0.1975 - Accuracy: 0.9251\n",
      "Epoch 5/5 - Loss: 0.1041 - Accuracy: 0.9645\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Evaluate the Model\n",
    "if y_test is not None:\n",
    "    model.eval()  \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for texts, labels in test_loader:\n",
    "            texts = texts.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(texts)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    prec = precision_score(all_labels, all_preds, average='weighted')\n",
    "    rec = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    print(\"LSTM Test Accuracy:\", acc)\n",
    "    print(\"LSTM Test Precision:\", prec)\n",
    "    print(\"LSTM Test Recall:\", rec)\n",
    "    print(\"LSTM Test F1-Score:\", f1)\n",
    "else:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_test_tensor.to(device))\n",
    "        _, predicted = torch.max(predictions, 1)\n",
    "    print(\"Sample predictions on test data:\", predicted.cpu().numpy()[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env1)",
   "language": "python",
   "name": "env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f397c95cc30>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle dataset shape: (20800, 5)\n",
      "ISOT dataset shape: (44898, 5)\n",
      "Combined dataset shape: (65698, 7)\n",
      "\n",
      "Class distribution:\n",
      "label\n",
      "0    33868\n",
      "1    31830\n",
      "Name: count, dtype: int64\n",
      "Percentage of fake news: 51.55%\n",
      "Percentage of true news: 48.45%\n"
     ]
    }
   ],
   "source": [
    "# 2. Load and preprocess datasets\n",
    "# Load the Kaggle dataset\n",
    "kaggle_df = pd.read_csv('Data/Data.csv')\n",
    "\n",
    "# Load the ISOT datasets\n",
    "true_df = pd.read_csv('Data/True.csv')\n",
    "fake_df = pd.read_csv('Data/Fake.csv')\n",
    "\n",
    "# For ISOT, assign labels: true news = 1 and fake news = 0\n",
    "true_df['label'] = 1\n",
    "fake_df['label'] = 0\n",
    "\n",
    "# Combine the two ISOT datasets\n",
    "isot_df = pd.concat([true_df, fake_df], ignore_index=True)\n",
    "\n",
    "# Print dataset shapes\n",
    "print(\"Kaggle dataset shape:\", kaggle_df.shape)\n",
    "print(\"ISOT dataset shape:\", isot_df.shape)\n",
    "\n",
    "# Combine Kaggle and ISOT datasets\n",
    "df = pd.concat([kaggle_df, isot_df], ignore_index=True)\n",
    "\n",
    "# Shuffle the combined dataset\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "print(\"Combined dataset shape:\", df.shape)\n",
    "\n",
    "# Display class distribution\n",
    "print(\"\\nClass distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "print(f\"Percentage of fake news: {df['label'].value_counts()[0]/len(df)*100:.2f}%\")\n",
    "print(f\"Percentage of true news: {df['label'].value_counts()[1]/len(df)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample raw content:\n",
      "‘We’re Just Not Into Her’ – Hillary Clinton Losing Support of Millennials Patrick Henningsen  21st Century WireWhat s up with the millennials? The Clinton campaign may have felt a jolt after seeing th\n",
      "\n",
      "Sample cleaned content:\n",
      "we re just not into her hillary clinton losing support of millennials patrick henningsen 21st century wirewhat s up with the millennials the clinton campaign may have felt a jolt after seeing the fron\n"
     ]
    }
   ],
   "source": [
    "# 3. Handle missing values and create content column\n",
    "# Drop rows missing 'text' (critical for classification)\n",
    "df = df.dropna(subset=['text'])\n",
    "\n",
    "# Fill missing 'title' values with a placeholder\n",
    "df['title'] = df['title'].fillna(\"No Title Provided\")\n",
    "\n",
    "# If there is an 'author' column, fill missing values with \"Unknown\"\n",
    "if 'author' in df.columns:\n",
    "    df['author'] = df['author'].fillna(\"Unknown\")\n",
    "\n",
    "# Create combined content field\n",
    "if 'content' not in df.columns:\n",
    "    df['content'] = df['title'] + \" \" + df['text']\n",
    "\n",
    "# Text cleaning function - keep it simple but effective\n",
    "def clean_text(text):\n",
    "    \"\"\"Basic preprocessing of text\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)  # Replace punctuation with space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespace\n",
    "    return text\n",
    "\n",
    "# Apply text cleaning\n",
    "df['cleaned_content'] = df['content'].apply(clean_text)\n",
    "\n",
    "# Print a sample to verify\n",
    "print(\"\\nSample raw content:\")\n",
    "print(df['content'].iloc[0][:200])\n",
    "print(\"\\nSample cleaned content:\")\n",
    "print(df['cleaned_content'].iloc[0][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 47274 samples\n",
      "Validation set: 5253 samples\n",
      "Test set: 13132 samples\n",
      "Total unique words found: 200245\n",
      "Vocabulary size: 15002\n",
      "Training tensor shape: torch.Size([47274, 300])\n",
      "Validation tensor shape: torch.Size([5253, 300])\n",
      "Test tensor shape: torch.Size([13132, 300])\n",
      "Batch contains 3 elements\n",
      "Batch verification: texts shape: torch.Size([64, 300]), lengths shape: torch.Size([64]), labels shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# 4. Split into training, validation, and testing sets\n",
    "X = df['cleaned_content']\n",
    "y = df['label']\n",
    "\n",
    "# Split 80% training, 20% testing\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Further split training data into training and validation sets (90% train, 10% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.1, random_state=42, stratify=y_train_full)\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Validation set: {len(X_val)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "\n",
    "# Simple but fast tokenizer function\n",
    "def tokenize(text, max_words=None):\n",
    "    tokens = text.split()\n",
    "    if max_words:\n",
    "        tokens = tokens[:max_words]\n",
    "    return tokens\n",
    "\n",
    "# Build vocabulary with better coverage (using top 15,000 words)\n",
    "all_tokens = []\n",
    "for text in X_train:\n",
    "    all_tokens.extend(tokenize(text))\n",
    "\n",
    "word_counts = Counter(all_tokens)\n",
    "print(f\"Total unique words found: {len(word_counts)}\")\n",
    "\n",
    "# Create vocabulary (0: padding, 1: unknown)\n",
    "MAX_VOCAB_SIZE = 15000  # Increased from original implementation\n",
    "MIN_WORD_FREQ = 3       # Only include words that appear at least 3 times\n",
    "\n",
    "vocab = {'<pad>': 0, '<unk>': 1}\n",
    "idx = 2\n",
    "for word, count in word_counts.most_common(MAX_VOCAB_SIZE):\n",
    "    if count >= MIN_WORD_FREQ:\n",
    "        vocab[word] = idx\n",
    "        idx += 1\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# For inverse lookups\n",
    "idx_to_word = {idx: word for word, idx in vocab.items()}\n",
    "\n",
    "# Convert text to sequences\n",
    "def text_to_sequence(text, vocab, max_len=300):\n",
    "    tokens = tokenize(text, max_len)\n",
    "    sequence = [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
    "    \n",
    "    # Get actual length before padding\n",
    "    length = len(sequence)\n",
    "    \n",
    "    # Pad sequence to uniform length\n",
    "    if len(sequence) < max_len:\n",
    "        sequence += [vocab['<pad>']] * (max_len - len(sequence))\n",
    "    else:\n",
    "        sequence = sequence[:max_len]\n",
    "        length = max_len\n",
    "        \n",
    "    return sequence, length\n",
    "\n",
    "MAX_LEN = 300\n",
    "X_train_seqs = []\n",
    "X_train_lengths = []\n",
    "for text in X_train:\n",
    "    seq, length = text_to_sequence(text, vocab, MAX_LEN)\n",
    "    X_train_seqs.append(seq)\n",
    "    X_train_lengths.append(length)\n",
    "\n",
    "# Now do the same for validation set\n",
    "X_val_seqs = []\n",
    "X_val_lengths = []\n",
    "for text in X_val:\n",
    "    seq, length = text_to_sequence(text, vocab, MAX_LEN)\n",
    "    X_val_seqs.append(seq)\n",
    "    X_val_lengths.append(length)\n",
    "\n",
    "X_test_seqs = []\n",
    "X_test_lengths = []\n",
    "for text in X_test:\n",
    "    seq, length = text_to_sequence(text, vocab, MAX_LEN)\n",
    "    X_test_seqs.append(seq)\n",
    "    X_test_lengths.append(length)\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.LongTensor(X_train_seqs)\n",
    "X_train_lengths = torch.LongTensor(X_train_lengths)\n",
    "y_train_tensor = torch.LongTensor(y_train.values)\n",
    "\n",
    "X_val_tensor = torch.LongTensor(X_val_seqs)\n",
    "X_val_lengths = torch.LongTensor(X_val_lengths)\n",
    "y_val_tensor = torch.LongTensor(y_val.values)\n",
    "\n",
    "X_test_tensor = torch.LongTensor(X_test_seqs)\n",
    "X_test_lengths = torch.LongTensor(X_test_lengths)\n",
    "y_test_tensor = torch.LongTensor(y_test.values)\n",
    "\n",
    "print(f\"Training tensor shape: {X_train_tensor.shape}\")\n",
    "print(f\"Validation tensor shape: {X_val_tensor.shape}\")\n",
    "print(f\"Test tensor shape: {X_test_tensor.shape}\")\n",
    "\n",
    "# Create custom dataset class\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, lengths, labels):\n",
    "        self.texts = texts\n",
    "        self.lengths = lengths\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.lengths[idx], self.labels[idx]\n",
    "\n",
    "# Create datasets - now with sequence lengths included\n",
    "train_dataset = NewsDataset(X_train_tensor, X_train_lengths, y_train_tensor)\n",
    "val_dataset = NewsDataset(X_val_tensor, X_val_lengths, y_val_tensor)\n",
    "test_dataset = NewsDataset(X_test_tensor, X_test_lengths, y_test_tensor)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Verify the DataLoader returns the correct structure\n",
    "for batch in train_loader:\n",
    "    print(f\"Batch contains {len(batch)} elements\")\n",
    "    texts, lengths, labels = batch\n",
    "    print(f\"Batch verification: texts shape: {texts.shape}, lengths shape: {lengths.shape}, labels shape: {labels.shape}\")\n",
    "    break  # Only check the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training batches: 333\n",
      "Number of validation batches: 37\n",
      "Number of test batches: 103\n"
     ]
    }
   ],
   "source": [
    "# 5. Create Dataset, DataLoader, and add augmentation\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, sequences, seq_lengths, labels, augment=False):\n",
    "        self.sequences = sequences\n",
    "        self.seq_lengths = seq_lengths\n",
    "        self.labels = labels\n",
    "        self.augment = augment\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        length = self.seq_lengths[idx]\n",
    "        \n",
    "        # Apply simple data augmentation (word dropout) if enabled\n",
    "        if self.augment and length > 10:  # Don't augment very short sequences\n",
    "            dropout_mask = torch.rand(len(seq)) > 0.1  # 10% word dropout\n",
    "            # Keep padding as is (don't dropout padding)\n",
    "            dropout_mask = dropout_mask | (seq == 0)\n",
    "            # Apply dropout and replace dropped tokens with <unk>\n",
    "            seq = torch.where(dropout_mask, seq, torch.tensor(1, dtype=torch.long))\n",
    "            \n",
    "        return seq, length, self.labels[idx]\n",
    "\n",
    "# Create datasets with stratification\n",
    "# First, create a validation split from the training data\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train_tensor, y_train_tensor, test_size=0.1, stratify=y_train_tensor, random_state=42\n",
    ")\n",
    "\n",
    "X_train_lengths_final = X_train_lengths[:len(X_train_final)]\n",
    "X_val_lengths = X_train_lengths[len(X_train_final):]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = NewsDataset(X_train_final, X_train_lengths_final, y_train_final, augment=True)\n",
    "val_dataset = NewsDataset(X_val, X_val_lengths, y_val, augment=False)\n",
    "test_dataset = NewsDataset(X_test_tensor, X_test_lengths, y_test_tensor, augment=False)\n",
    "\n",
    "# Create data loaders with variable batch sizes based on dataset size\n",
    "BATCH_SIZE = 128  # Larger batch size for stability\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=0, pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=0, pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=0, pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of validation batches: {len(val_loader)}\")\n",
    "print(f\"Number of test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImprovedRNN model defined successfully\n"
     ]
    }
   ],
   "source": [
    "# 6. Define an improved RNN model with multi-head self-attention\n",
    "class ImprovedRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers=2, dropout=0.5):\n",
    "        super(ImprovedRNN, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Use GRU (a gated RNN) for better training dynamics (still a type of RNN)\n",
    "        self.rnn = nn.GRU(embedding_dim, \n",
    "                          hidden_dim, \n",
    "                          num_layers=n_layers, \n",
    "                          bidirectional=True, \n",
    "                          batch_first=True,\n",
    "                          dropout=dropout if n_layers > 1 else 0)\n",
    "        \n",
    "        # LayerNorm for stable training\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim*2)\n",
    "        \n",
    "        # Multi-head self-attention block (captures richer interactions)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=hidden_dim*2, num_heads=8, dropout=0.1, batch_first=True)\n",
    "        \n",
    "        # Fully connected layers with batch normalization and GELU activations\n",
    "        self.fc1 = nn.Linear(hidden_dim*2, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim//2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim//2)\n",
    "        self.fc3 = nn.Linear(hidden_dim//2, output_dim)\n",
    "        \n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    def init_weights(self):\n",
    "        # Xavier initialization for weight matrices, uniform for biases\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                if param.dim() > 1:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                else:\n",
    "                    nn.init.uniform_(param, -0.1, 0.1)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, max_len]\n",
    "        embedded = self.dropout(self.embedding(x))  # [B, L, emb_dim]\n",
    "        \n",
    "        # GRU layer\n",
    "        rnn_out, _ = self.rnn(embedded)  # [B, L, hidden_dim*2]\n",
    "        \n",
    "        # Layer normalization\n",
    "        normed = self.layer_norm(rnn_out)\n",
    "        \n",
    "        # Create key padding mask (True for padding positions)\n",
    "        mask = (x == 0)\n",
    "        \n",
    "        # Multi-head self-attention; outputs same shape as input\n",
    "        attn_out, _ = self.attention(normed, normed, normed, key_padding_mask=mask)\n",
    "        \n",
    "        # Residual connection\n",
    "        context = attn_out + normed   # [B, L, hidden_dim*2]\n",
    "        \n",
    "        # Average pooling over the time dimension—weighting non-pad tokens only\n",
    "        # mask: [B, L] -> ~mask gives valid tokens\n",
    "        valid = (~mask).unsqueeze(2).float()  # [B, L, 1]\n",
    "        lengths = valid.sum(dim=1)  # [B, 1]\n",
    "        pooled = (context * valid).sum(dim=1) / (lengths + 1e-8)  # [B, hidden_dim*2]\n",
    "        \n",
    "        # Fully connected classifier\n",
    "        out = self.dropout(pooled)\n",
    "        out = self.fc1(out)\n",
    "        out = self.bn1(out)\n",
    "        out = self.activation(out)\n",
    "        \n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.activation(out)\n",
    "        \n",
    "        out = self.dropout(out)\n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "print(\"ImprovedRNN model defined successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "ImprovedRNN(\n",
      "  (embedding): Embedding(15002, 300, padding_idx=0)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (rnn): GRU(300, 384, num_layers=2, batch_first=True, dropout=0.4, bidirectional=True)\n",
      "  (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (fc1): Linear(in_features=768, out_features=384, bias=True)\n",
      "  (bn1): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=384, out_features=192, bias=True)\n",
      "  (bn2): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=192, out_features=2, bias=True)\n",
      "  (activation): GELU(approximate='none')\n",
      ")\n",
      "Trainable parameters: 11,474,618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koult/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 7. Initialize model (updated hyperparameters for stability)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 384\n",
    "OUTPUT_DIM = 2\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.4\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = ImprovedRNN(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    output_dim=OUTPUT_DIM,\n",
    "    n_layers=N_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ")\n",
    "model.init_weights()\n",
    "model = model.to(device)\n",
    "\n",
    "# Remove label smoothing for stability and lower LR\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "print(model)\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 5 epochs...\n",
      "Epoch 1/5 | Batch 0/333 | Loss: 0.6927 | Acc: 0.5000\n",
      "NaN loss encountered. Skipping this batch.\n",
      "Epoch 1/5 | Batch 50/333 | Loss: 0.4022 | Acc: 0.9062\n",
      "Epoch 1/5 | Batch 100/333 | Loss: 0.3607 | Acc: 0.8359\n",
      "Epoch 1/5 | Batch 150/333 | Loss: 0.2245 | Acc: 0.9375\n",
      "NaN loss encountered. Skipping this batch.\n",
      "Epoch 1/5 | Batch 200/333 | Loss: 0.2287 | Acc: 0.9141\n",
      "NaN loss encountered. Skipping this batch.\n",
      "Epoch 1/5 | Batch 250/333 | Loss: 0.2611 | Acc: 0.8984\n",
      "NaN loss encountered. Skipping this batch.\n",
      "NaN loss encountered. Skipping this batch.\n",
      "Epoch 1/5 | Batch 300/333 | Loss: 0.1464 | Acc: 0.9453\n",
      "NaN loss encountered. Skipping this batch.\n",
      "\n",
      "Epoch 1/5 summary:\n",
      "  Train Loss: 0.2783 | Train Acc: 0.8830 | Train F1: 0.8990\n",
      "------------------------------------------------------------\n",
      "Epoch 2/5 | Batch 0/333 | Loss: 0.1189 | Acc: 0.9766\n",
      "Epoch 2/5 | Batch 50/333 | Loss: 0.1184 | Acc: 0.9531\n",
      "NaN loss encountered. Skipping this batch.\n",
      "Epoch 2/5 | Batch 100/333 | Loss: 0.1696 | Acc: 0.9453\n",
      "NaN loss encountered. Skipping this batch.\n",
      "Epoch 2/5 | Batch 150/333 | Loss: 0.1648 | Acc: 0.9297\n",
      "NaN loss encountered. Skipping this batch.\n",
      "NaN loss encountered. Skipping this batch.\n",
      "Epoch 2/5 | Batch 200/333 | Loss: 0.0820 | Acc: 0.9609\n",
      "NaN loss encountered. Skipping this batch.\n",
      "Epoch 2/5 | Batch 250/333 | Loss: 0.2084 | Acc: 0.9062\n",
      "NaN loss encountered. Skipping this batch.\n",
      "Epoch 2/5 | Batch 300/333 | Loss: 0.0699 | Acc: 0.9609\n",
      "\n",
      "Epoch 2/5 summary:\n",
      "  Train Loss: 0.1094 | Train Acc: 0.9428 | Train F1: 0.9601\n",
      "------------------------------------------------------------\n",
      "NaN loss encountered. Skipping this batch.\n",
      "Epoch 3/5 | Batch 50/333 | Loss: 0.0556 | Acc: 0.9766\n",
      "NaN loss encountered. Skipping this batch.\n",
      "Epoch 3/5 | Batch 100/333 | Loss: 0.0388 | Acc: 0.9844\n",
      "NaN loss encountered. Skipping this batch.\n",
      "NaN loss encountered. Skipping this batch.\n",
      "Epoch 3/5 | Batch 150/333 | Loss: 0.0705 | Acc: 0.9766\n",
      "Epoch 3/5 | Batch 200/333 | Loss: 0.0413 | Acc: 0.9844\n",
      "NaN loss encountered. Skipping this batch.\n",
      "NaN loss encountered. Skipping this batch.\n",
      "Epoch 3/5 | Batch 250/333 | Loss: 0.0745 | Acc: 0.9688\n",
      "Epoch 3/5 | Batch 300/333 | Loss: 0.0430 | Acc: 0.9922\n",
      "\n",
      "Epoch 3/5 summary:\n",
      "  Train Loss: 0.0708 | Train Acc: 0.9576 | Train F1: 0.9752\n",
      "------------------------------------------------------------\n",
      "Epoch 4/5 | Batch 0/333 | Loss: 0.0381 | Acc: 0.9922\n",
      "NaN loss encountered. Skipping this batch.\n",
      "Epoch 4/5 | Batch 50/333 | Loss: 0.0428 | Acc: 0.9844\n",
      "NaN loss encountered. Skipping this batch.\n",
      "NaN loss encountered. Skipping this batch.\n",
      "Epoch 4/5 | Batch 100/333 | Loss: 0.0406 | Acc: 0.9766\n",
      "NaN loss encountered. Skipping this batch.\n",
      "NaN loss encountered. Skipping this batch.\n",
      "Epoch 4/5 | Batch 150/333 | Loss: 0.0185 | Acc: 0.9922\n",
      "Epoch 4/5 | Batch 200/333 | Loss: 0.0373 | Acc: 0.9844\n",
      "Epoch 4/5 | Batch 250/333 | Loss: 0.0915 | Acc: 0.9609\n",
      "NaN loss encountered. Skipping this batch.\n",
      "Epoch 4/5 | Batch 300/333 | Loss: 0.0209 | Acc: 1.0000\n",
      "\n",
      "Epoch 4/5 summary:\n",
      "  Train Loss: 0.0530 | Train Acc: 0.9638 | Train F1: 0.9815\n",
      "------------------------------------------------------------\n",
      "Epoch 5/5 | Batch 0/333 | Loss: 0.0783 | Acc: 0.9609\n",
      "NaN loss encountered. Skipping this batch.\n",
      "Epoch 5/5 | Batch 50/333 | Loss: 0.0282 | Acc: 0.9922\n",
      "NaN loss encountered. Skipping this batch.\n",
      "Epoch 5/5 | Batch 100/333 | Loss: 0.0209 | Acc: 1.0000\n",
      "NaN loss encountered. Skipping this batch.\n",
      "Epoch 5/5 | Batch 150/333 | Loss: 0.0722 | Acc: 0.9609\n",
      "Epoch 5/5 | Batch 200/333 | Loss: 0.1888 | Acc: 0.9531\n",
      "NaN loss encountered. Skipping this batch.\n",
      "NaN loss encountered. Skipping this batch.\n",
      "Epoch 5/5 | Batch 250/333 | Loss: 0.0817 | Acc: 0.9766\n",
      "Epoch 5/5 | Batch 300/333 | Loss: 0.0114 | Acc: 1.0000\n",
      "NaN loss encountered. Skipping this batch.\n",
      "\n",
      "Epoch 5/5 summary:\n",
      "  Train Loss: 0.0421 | Train Acc: 0.9677 | Train F1: 0.9855\n",
      "------------------------------------------------------------\n",
      "\n",
      "Test Evaluation:\n",
      "Test Accuracy:  0.5158\n",
      "Test Precision: 0.2661\n",
      "Test Recall:    0.5158\n",
      "Test F1-Score:  0.3511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koult/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "N_EPOCHS = 5  # Number of epochs\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "train_f1s = []\n",
    "\n",
    "print(f\"Starting training for {N_EPOCHS} epochs...\")\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    all_train_preds = []\n",
    "    all_train_labels = []\n",
    "    \n",
    "    for batch_idx, (texts, lengths, labels) in enumerate(train_loader):\n",
    "        texts, lengths, labels = texts.to(device), lengths.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(texts)\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        # Check for NaN loss\n",
    "        if torch.isnan(loss):\n",
    "            print(\"NaN loss encountered. Skipping this batch.\")\n",
    "            continue\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update metrics\n",
    "        _, predicted = torch.max(predictions, 1)\n",
    "        correct = (predicted == labels).float().sum()\n",
    "        accuracy = correct / len(labels)\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += accuracy.item()\n",
    "        all_train_preds.extend(predicted.cpu().numpy())\n",
    "        all_train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{N_EPOCHS} | Batch {batch_idx}/{len(train_loader)} | Loss: {loss.item():.4f} | Acc: {accuracy.item():.4f}\")\n",
    "    \n",
    "    # Compute and record average train metrics\n",
    "    train_loss = epoch_loss / len(train_loader)\n",
    "    train_acc = epoch_acc / len(train_loader)\n",
    "    train_f1 = f1_score(all_train_labels, all_train_preds, average='weighted')\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    train_f1s.append(train_f1)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{N_EPOCHS} summary:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Train F1: {train_f1:.4f}\")\n",
    "    print('-' * 60)\n",
    "\n",
    "# After training, evaluate on the test set and calculate test metrics\n",
    "model.eval()\n",
    "all_test_preds = []\n",
    "all_test_labels = []\n",
    "with torch.no_grad():\n",
    "    for texts, lengths, labels in test_loader:\n",
    "        texts, lengths, labels = texts.to(device), lengths.to(device), labels.to(device)\n",
    "        predictions = model(texts)\n",
    "        _, predicted = torch.max(predictions, 1)\n",
    "        all_test_preds.extend(predicted.cpu().numpy())\n",
    "        all_test_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_accuracy = accuracy_score(all_test_labels, all_test_preds)\n",
    "test_precision = precision_score(all_test_labels, all_test_preds, average='weighted')\n",
    "test_recall = recall_score(all_test_labels, all_test_preds, average='weighted')\n",
    "test_f1 = f1_score(all_test_labels, all_test_preds, average='weighted')\n",
    "\n",
    "print(\"\\nTest Evaluation:\")\n",
    "print(f\"Test Accuracy:  {test_accuracy:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Recall:    {test_recall:.4f}\")\n",
    "print(f\"Test F1-Score:  {test_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

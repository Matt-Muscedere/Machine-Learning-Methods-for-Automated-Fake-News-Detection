{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48330b27-410b-41a2-9f82-6d3440cc3ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daa307f1-6715-4b4d-be23-be992a328a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load the Datasets\n",
    "# Load the Kaggle dataset\n",
    "kaggle_df = pd.read_csv('Data/Data.csv')\n",
    "\n",
    "# Load the ISOT datasets\n",
    "true_df = pd.read_csv('Data/True.csv')\n",
    "fake_df = pd.read_csv('Data/Fake.csv')\n",
    "\n",
    "# For ISOT, assign labels: assume true news = 1 and fake news = 0\n",
    "true_df['label'] = 1\n",
    "fake_df['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bba1e7cd-e2d0-487e-a84b-8215f723ab53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle dataset shape: (20800, 5)\n",
      "ISOT dataset shape: (44898, 5)\n",
      "Combined dataset shape: (65698, 7)\n"
     ]
    }
   ],
   "source": [
    "# 3. Combine the Datasets\n",
    "# Combine the two ISOT datasets\n",
    "isot_df = pd.concat([true_df, fake_df], ignore_index=True)\n",
    "\n",
    "# Optionally, inspect shapes\n",
    "print(\"Kaggle dataset shape:\", kaggle_df.shape)\n",
    "print(\"ISOT dataset shape:\", isot_df.shape)\n",
    "\n",
    "# Combine Kaggle and ISOT data into one DataFrame\n",
    "df = pd.concat([kaggle_df, isot_df], ignore_index=True)\n",
    "\n",
    "# Shuffle the combined dataset\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "print(\"Combined dataset shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d40631b-7d76-4d96-8e64-b43e4cd28ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Handle Missing Values\n",
    "# Drop rows missing 'text' (critical for classification)\n",
    "df = df.dropna(subset=['text'])\n",
    "\n",
    "# Fill missing 'title' values with a placeholder\n",
    "df['title'] = df['title'].fillna(\"No Title Provided\")\n",
    "\n",
    "# If there is an 'author' column, fill missing values with \"Unknown\"\n",
    "if 'author' in df.columns:\n",
    "    df['author'] = df['author'].fillna(\"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9f81bc4-7479-44dc-bb5d-b95e142cc953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Create the 'content' Column\n",
    "# Combine 'title' and 'text' into a single text field\n",
    "if 'content' not in df.columns:\n",
    "    df['content'] = df['title'] + \" \" + df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94eb5946-b06a-40f5-8823-972f6e32eb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Split into Training and Testing Sets\n",
    "X = df['content']\n",
    "y = df['label']\n",
    "\n",
    "# Split the data (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e539c447-9a60-4872-bed9-8f3b4616ad15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Preprocess Text for the LSTM\n",
    "\n",
    "def tokenize(text):\n",
    "    # Simple whitespace and word-boundary tokenization (lowercase)\n",
    "    text = text.lower()\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "    return tokens\n",
    "\n",
    "# Build vocabulary from training texts\n",
    "all_tokens = []\n",
    "for text in X_train:\n",
    "    all_tokens.extend(tokenize(text))\n",
    "\n",
    "freq = Counter(all_tokens)\n",
    "# Create vocabulary: reserve index 0 for padding, 1 for unknown tokens\n",
    "vocab = {word: i+2 for i, (word, count) in enumerate(freq.items())}\n",
    "vocab_size = len(vocab) + 2\n",
    "\n",
    "def text_to_sequence(text, vocab):\n",
    "    tokens = tokenize(text)\n",
    "    return [vocab.get(token, 1) for token in tokens]  # 1 for unknown\n",
    "\n",
    "# Convert texts to sequences\n",
    "X_train_seq = [text_to_sequence(text, vocab) for text in X_train]\n",
    "X_test_seq = [text_to_sequence(text, vocab) for text in X_test]\n",
    "\n",
    "# Pad sequences to a fixed length\n",
    "max_len = 500  # You can adjust this as needed\n",
    "def pad_sequence(seq, max_len):\n",
    "    if len(seq) < max_len:\n",
    "        return seq + [0]*(max_len - len(seq))\n",
    "    else:\n",
    "        return seq[:max_len]\n",
    "\n",
    "X_train_pad = [pad_sequence(seq, max_len) for seq in X_train_seq]\n",
    "X_test_pad = [pad_sequence(seq, max_len) for seq in X_test_seq]\n",
    "\n",
    "# Convert lists to torch tensors\n",
    "X_train_tensor = torch.tensor(X_train_pad, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test_pad, dtype=torch.long)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5198ec2-8fd5-48cb-a33d-602885bd8ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Create a PyTorch Dataset and DataLoader\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "\n",
    "batch_size = 64\n",
    "train_dataset = NewsDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = NewsDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d19deb77-cf6f-4e50-b5b2-8f2995898d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Define the LSTM Model\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers=1):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_length]\n",
    "        embedded = self.embedding(x)                # [batch_size, seq_length, embedding_dim]\n",
    "        lstm_out, _ = self.lstm(embedded)             # [batch_size, seq_length, hidden_dim]\n",
    "        last_hidden = lstm_out[:, -1, :]              # Use the last hidden state\n",
    "        output = self.fc(last_hidden)                 # [batch_size, output_dim]\n",
    "        return output\n",
    "\n",
    "embedding_dim = 100\n",
    "hidden_dim = 128\n",
    "output_dim = 2  # Binary classification (0 or 1)\n",
    "\n",
    "model = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8b33374-d198-4838-9f92-a7c5bd388855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Loss: 0.6905 - Accuracy: 0.5252\n",
      "Epoch 2/5 - Loss: 0.6163 - Accuracy: 0.6426\n",
      "Epoch 3/5 - Loss: 0.2870 - Accuracy: 0.8902\n",
      "Epoch 4/5 - Loss: 0.1177 - Accuracy: 0.9575\n",
      "Epoch 5/5 - Loss: 0.0600 - Accuracy: 0.9804\n"
     ]
    }
   ],
   "source": [
    "# 10. Train the LSTM Model on GPU\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for texts, labels in train_loader:\n",
    "        texts = texts.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(texts)  # [batch_size, output_dim]\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * texts.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f} - Accuracy: {epoch_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e8568fc-ef05-4426-9396-fd7836640db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9656564118184587\n"
     ]
    }
   ],
   "source": [
    "# 11. Evaluate the Model on the Test Set\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for texts, labels in test_loader:\n",
    "        texts = texts.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(texts)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = correct / total\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0771ed0d-53e3-40ef-a152-44d23557f0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved Model - Epoch 1/5 - Loss: 0.2423 - Accuracy: 0.8995\n",
      "Improved Model - Epoch 2/5 - Loss: 0.1073 - Accuracy: 0.9617\n",
      "Improved Model - Epoch 3/5 - Loss: 0.0673 - Accuracy: 0.9784\n",
      "Improved Model - Epoch 4/5 - Loss: 0.0448 - Accuracy: 0.9862\n",
      "Improved Model - Epoch 5/5 - Loss: 0.0317 - Accuracy: 0.9904\n",
      "Improved Model - Test Accuracy: 0.9687785561985989\n"
     ]
    }
   ],
   "source": [
    "# Define an improved LSTM model with stacking, bidirectionality, and dropout\n",
    "class ImprovedLSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers=2, bidirectional=True, dropout=0.5):\n",
    "        super(ImprovedLSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        # Using a 2-layer bidirectional LSTM with dropout applied between layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, \n",
    "            hidden_dim, \n",
    "            num_layers=num_layers, \n",
    "            batch_first=True, \n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        # If bidirectional, output dimension doubles\n",
    "        lstm_out_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        # Extra fully-connected layer before the final output layer\n",
    "        self.fc1 = nn.Linear(lstm_out_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_length]\n",
    "        embedded = self.embedding(x)  # [batch_size, seq_length, embedding_dim]\n",
    "        lstm_out, _ = self.lstm(embedded)  # [batch_size, seq_length, hidden_dim*(2 if bidirectional else 1)]\n",
    "        \n",
    "        # For bidirectional LSTM, concatenate the last hidden state from the forward and the first from the backward pass.\n",
    "        if self.lstm.bidirectional:\n",
    "            # Forward LSTM: take the last time step; Backward LSTM: take the first time step\n",
    "            forward_hidden = lstm_out[:, -1, :self.lstm.hidden_size]\n",
    "            backward_hidden = lstm_out[:, 0, self.lstm.hidden_size:]\n",
    "            last_hidden = torch.cat((forward_hidden, backward_hidden), dim=1)\n",
    "        else:\n",
    "            last_hidden = lstm_out[:, -1, :]\n",
    "            \n",
    "        x = self.fc1(last_hidden)\n",
    "        x = self.dropout(x)\n",
    "        output = self.fc2(x)\n",
    "        return output\n",
    "\n",
    "# Set device to use GPU index 2 (for example)\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the improved model and move it to the specified device\n",
    "improved_model = ImprovedLSTMClassifier(vocab_size, embedding_dim, hidden_dim, output_dim, num_layers=2, bidirectional=True, dropout=0.5)\n",
    "improved_model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(improved_model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    improved_model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for texts, labels in train_loader:\n",
    "        texts = texts.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = improved_model(texts)  # [batch_size, output_dim]\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * texts.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    print(f\"Improved Model - Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f} - Accuracy: {epoch_acc:.4f}\")\n",
    "\n",
    "# Evaluate the Improved LSTM Model on the Test Set\n",
    "improved_model.eval()  # Set model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for texts, labels in test_loader:\n",
    "        texts = texts.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = improved_model(texts)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = correct / total\n",
    "print(\"Improved Model - Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1c4d25-e3be-473d-aa91-aec0182a879a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env1)",
   "language": "python",
   "name": "env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

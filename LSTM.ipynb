{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48330b27-410b-41a2-9f82-6d3440cc3ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa307f1-6715-4b4d-be23-be992a328a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load the Datasets\n",
    "# Load the Kaggle dataset\n",
    "kaggle_df = pd.read_csv('Data/Data.csv')\n",
    "\n",
    "# Load the ISOT datasets\n",
    "true_df = pd.read_csv('Data/True.csv')\n",
    "fake_df = pd.read_csv('Data/Fake.csv')\n",
    "\n",
    "# For ISOT, assign labels\n",
    "true_df['label'] = 1\n",
    "fake_df['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba1e7cd-e2d0-487e-a84b-8215f723ab53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle dataset shape: (20800, 5)\n",
      "ISOT dataset shape: (44898, 5)\n",
      "Combined dataset shape: (65698, 7)\n"
     ]
    }
   ],
   "source": [
    "# 3. Combine the Datasets\n",
    "# Combine the two ISOT datasets\n",
    "isot_df = pd.concat([true_df, fake_df], ignore_index=True)\n",
    "\n",
    "print(\"Kaggle dataset shape:\", kaggle_df.shape)\n",
    "print(\"ISOT dataset shape:\", isot_df.shape)\n",
    "\n",
    "# Combine Kaggle and ISOT data into one DataFrame\n",
    "df = pd.concat([kaggle_df, isot_df], ignore_index=True)\n",
    "\n",
    "# Shuffle the combined dataset\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "print(\"Combined dataset shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d40631b-7d76-4d96-8e64-b43e4cd28ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Handle Missing Values\n",
    "# Drop rows missing 'text'\n",
    "df = df.dropna(subset=['text'])\n",
    "\n",
    "# Fill missing 'title' values with a placeholder\n",
    "df['title'] = df['title'].fillna(\"No Title Provided\")\n",
    "\n",
    "if 'author' in df.columns:\n",
    "    df['author'] = df['author'].fillna(\"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9f81bc4-7479-44dc-bb5d-b95e142cc953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Create the 'content' Column\n",
    "# Combine 'title' and 'text' into a single text field\n",
    "if 'content' not in df.columns:\n",
    "    df['content'] = df['title'] + \" \" + df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94eb5946-b06a-40f5-8823-972f6e32eb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Split into Training and Testing Sets\n",
    "X = df['content']\n",
    "y = df['label']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e539c447-9a60-4872-bed9-8f3b4616ad15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Preprocess Text for the LSTM\n",
    "\n",
    "def tokenize(text):\n",
    "    # Convert text to lowercase to ensure case insensitivity.\n",
    "    text = text.lower()\n",
    "    # Use regex to split the text into tokens based on word boundaries.\n",
    "    # This will match words consisting of alphanumeric characters.\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "    return tokens\n",
    "\n",
    "# Build a vocabulary from the training texts.\n",
    "all_tokens = []\n",
    "# Loop over each training text and accumulate all tokens.\n",
    "for text in X_train:\n",
    "    all_tokens.extend(tokenize(text))\n",
    "\n",
    "# Count the frequency of each token in the training set.\n",
    "freq = Counter(all_tokens)\n",
    "# Build a vocabulary mapping where each word is assigned a unique integer.\n",
    "# Reserve index 0 for padding and index 1 for tokens that are not found (unknown).\n",
    "vocab = {word: i+2 for i, (word, count) in enumerate(freq.items())}\n",
    "# Compute the total vocabulary size including the reserved indices.\n",
    "vocab_size = len(vocab) + 2\n",
    "\n",
    "def text_to_sequence(text, vocab):\n",
    "    # Tokenize the text using our defined function.\n",
    "    tokens = tokenize(text)\n",
    "    # Convert each token into its corresponding integer based on the vocabulary.\n",
    "    # If the token is not found in the vocabulary, assign it the unknown token index (1).\n",
    "    return [vocab.get(token, 1) for token in tokens]\n",
    "\n",
    "# Convert all texts in both training and testing sets into sequences of integers.\n",
    "X_train_seq = [text_to_sequence(text, vocab) for text in X_train]\n",
    "X_test_seq = [text_to_sequence(text, vocab) for text in X_test]\n",
    "\n",
    "# Define the maximum sequence length to standardize input sizes.\n",
    "max_len = 500\n",
    "\n",
    "def pad_sequence(seq, max_len):\n",
    "    # If the sequence is shorter than max_len, pad it with zeros (padding index).\n",
    "    if len(seq) < max_len:\n",
    "        return seq + [0]*(max_len - len(seq))\n",
    "    # If the sequence is longer, truncate it to max_len.\n",
    "    else:\n",
    "        return seq[:max_len]\n",
    "\n",
    "# Apply padding or truncation to every sequence in the training and test sets.\n",
    "X_train_pad = [pad_sequence(seq, max_len) for seq in X_train_seq]\n",
    "X_test_pad = [pad_sequence(seq, max_len) for seq in X_test_seq]\n",
    "\n",
    "# Convert the padded sequences into torch tensors with the appropriate data type.\n",
    "X_train_tensor = torch.tensor(X_train_pad, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test_pad, dtype=torch.long)\n",
    "\n",
    "# Convert the labels from the pandas Series into torch tensors for model training.\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5198ec2-8fd5-48cb-a33d-602885bd8ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Create a PyTorch Dataset and DataLoader\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "\n",
    "batch_size = 64\n",
    "train_dataset = NewsDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = NewsDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19deb77-cf6f-4e50-b5b2-8f2995898d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Define the LSTM Model\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers=1):\n",
    "        super().__init__()\n",
    "        # Embedding layer converts word indices into dense vectors.\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        # LSTM processes sequences of embeddings; batch_first=True means input shape is (batch, seq, feature).\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        # Fully connected layer maps the hidden state to output dimension (number of classes).\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Convert indices in x to embeddings.\n",
    "        x = self.embedding(x)\n",
    "        # Pass the embeddings through LSTM layer.\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # Select the output of the last time step for each sequence.\n",
    "        last_hidden = lstm_out[:, -1, :]\n",
    "        # Produce final logits via the fully connected layer.\n",
    "        return self.fc(last_hidden)\n",
    "\n",
    "# Hyperparameters: embedding size, hidden dimension, and output classes.\n",
    "embedding_dim = 100       # Size of each word vector.\n",
    "hidden_dim = 128          # Number of hidden units in LSTM.\n",
    "output_dim = 2            # Number of classes (e.g., fake vs. true news).\n",
    "\n",
    "# Instantiate the LSTM classifier model.\n",
    "model = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, output_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b33374-d198-4838-9f92-a7c5bd388855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Loss: 0.6655 - Accuracy: 0.5898\n",
      "Epoch 2/5 - Loss: 0.6697 - Accuracy: 0.5737\n",
      "Epoch 3/5 - Loss: 0.4905 - Accuracy: 0.7611\n",
      "Epoch 4/5 - Loss: 0.1975 - Accuracy: 0.9251\n",
      "Epoch 5/5 - Loss: 0.1041 - Accuracy: 0.9645\n"
     ]
    }
   ],
   "source": [
    "# The following code trains the LSTM model on a GPU if available.\n",
    "# It sets up device specification, loss function, optimizer, and iterates through training epochs.\n",
    "\n",
    "# Determine whether to use a specific GPU (cuda:2) or CPU if a GPU isn't available.\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set up the loss criteria and the optimizer.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "# Loop over epochs to train the model.\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Process each batch from the training data.\n",
    "    for texts, labels in train_loader:\n",
    "        # Move both inputs and labels to the designated device (GPU or CPU).\n",
    "        texts = texts.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Clear the gradients from the previous iteration.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass: compute the model outputs for the given texts.\n",
    "        outputs = model(texts)\n",
    "        \n",
    "        # Compute the cross-entropy loss between outputs and true labels.\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass: calculate gradients of the loss with respect to model parameters.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update model parameters based on computed gradients.\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Multiply the batch loss by the batch size and add to the running loss.\n",
    "        running_loss += loss.item() * texts.size(0)\n",
    "        \n",
    "        # Determine the predicted classes by selecting the index with the highest logit.\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        # Keep track of the total number of labels processed.\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        # Count the number of correct predictions.\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    # Compute average loss and accuracy for the current epoch.\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    \n",
    "    # Output the epoch's performance metrics.\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f} - Accuracy: {epoch_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205a318d-bb5a-4e31-8d7f-9d200ddbfb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Test Accuracy: 0.9604020712762718\n",
      "LSTM Test Precision: 0.9606478332134911\n",
      "LSTM Test Recall: 0.9604020712762718\n",
      "LSTM Test F1-Score: 0.9603782824660891\n"
     ]
    }
   ],
   "source": [
    "# Evaluation Cell for LSTM.ipynb\n",
    "\n",
    "# Import evaluation metrics for assessing classification performance\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Initialize empty lists to store the predicted outputs and the true labels for the test set\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# Set the model to evaluation mode so that layers like dropout behave correctly\n",
    "model.eval()  \n",
    "\n",
    "# Disable gradient computation since we are only doing inference\n",
    "with torch.no_grad():\n",
    "    # Iterate over all batches from the test loader\n",
    "    for texts, labels in test_loader:\n",
    "        # Move the texts and labels to the appropriate device (GPU or CPU)\n",
    "        texts = texts.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Compute the outputs from the model for the current batch\n",
    "        outputs = model(texts)\n",
    "        \n",
    "        # Determine the predicted class by selecting the one with the highest score\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Convert predictions and labels to numpy arrays and store them for evaluation\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "prec = precision_score(all_labels, all_preds, average='weighted')\n",
    "rec = recall_score(all_labels, all_preds, average='weighted')\n",
    "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "\n",
    "print(\"LSTM Test Accuracy:\", acc)\n",
    "print(\"LSTM Test Precision:\", prec)\n",
    "print(\"LSTM Test Recall:\", rec)\n",
    "print(\"LSTM Test F1-Score:\", f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env1)",
   "language": "python",
   "name": "env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
